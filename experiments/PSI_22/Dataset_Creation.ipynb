{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fafa90-d750-4449-a722-67504a3b6e1b",
   "metadata": {},
   "source": [
    "# Creating the dataset\n",
    "\n",
    "We are using data from JET. \n",
    "\n",
    "### Starting data \n",
    "\n",
    "- HRTS profiles \n",
    "    - Electron density and temperature, $n_e$, $T_e$\n",
    "    - HRTS Line of Sight coordinates, $R_{HRTS}$\n",
    "- Machine parameters\n",
    "    - $R$ [m], plasma major radius\n",
    "    - $a$ [m], plasma minor radius \n",
    "    - $V_P$ [m$^3$], total plasma volume enclosed by LCFS\n",
    "    - $\\delta_u,\\delta_l$ [-], upper and lower triangularities  \n",
    "    - $\\kappa$ [-], elongation, or ratio of height to width of plasma\n",
    "    - $P_{OHM}$ [W], ohmic power of the plasma\n",
    "    - $I_P$ [A], total plasma current enclosed by LCFS \n",
    "    - $B_T$ [T], total toroidal magnetic field\n",
    "    - $P_{NBI}$ [W], total neutral beam injected power into the plasma\n",
    "    - $P_{ICRH}$ [W], total ion cyclotron resonance heating imposed onto plasma\n",
    "    - $\\Gamma$ [e/s], fuelling rate of main isotope (aka gas puff)\n",
    "    - $q_{cycl}$ [-], safety factor, calculated from $q_{cycl} = \\frac{(1+2\\kappa^2)}{2}\\frac{2B_T\\pi a^2}{RI_P\\mu_0}$\n",
    "    \n",
    "#### Profile Information \n",
    "\n",
    "- Pulses/time windows found in JET pedestal database\n",
    "- Subset of pulses: \n",
    "    - ILW\n",
    "    - HRTS Validated \n",
    "    - Pulses with either no seeding or with nitrogen seeding\n",
    "    - Deuterium fuelling\n",
    "    - No kicks, rmps, or pellets\n",
    "    \n",
    "\n",
    "\n",
    "After filtering above: \n",
    "- Total number of pulses & time slices\n",
    "    - 1248\n",
    "   \n",
    "### Further slice filtering \n",
    "\n",
    "#### ELM Percents\n",
    "\n",
    "Slices which an ELM percentage could be calculated, using ELM timings from JET pedestal database\n",
    "\n",
    "\n",
    "After filtering: \n",
    "- Total number of slices from 608 pulses: \n",
    "    - 23499\n",
    "    \n",
    "#### $n_{e, sep}$\n",
    "\n",
    "- TODO: Describe process of gathering $n_{e, sep}$\n",
    "- Calculation is not perfect, so we remove slices that have an $n_{e, sep}$ approximation that falls outside of 1 standard deviation of the mean $n_{e, sep}$ for the pulse. \n",
    "\n",
    "- Total number of slices: \n",
    "    - 16111\n",
    "    \n",
    "## Profile padding \n",
    "\n",
    "The maximum length of the profiles is 19, so all will be padded to that value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a15d52-c539-4a9f-b6f5-3592a4e5d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy import interpolate\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections.abc import Iterable\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d191a8-158d-4551-8c25-a412f73bf2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_choice='SANDBOX_NO_VARIATIONS'\n",
    "file_loc='../../../moxie/data/processed/pedestal_profiles_ML_READY_ak_5052022_uncerts_mask.pickle'\n",
    "with open(file_loc, 'rb') as file:\n",
    "    massive_dict = pickle.load(file)\n",
    "full_dict = massive_dict[dataset_choice]\n",
    "massive_dict = {}\n",
    "with open('../../data/raw/new_elm_timings_catch.pickle', 'rb') as file: \n",
    "     JET_ELM_TIMINGS = pickle.load(file) \n",
    "JET_PDB = pd.read_csv('../../../moxie/data/processed/jet-pedestal-database.csv')\n",
    "PULSE_DF_SANDBOX = JET_PDB[(JET_PDB['FLAG:HRTSdatavalidated'] > 0) & (JET_PDB['shot'] > 80000) & (JET_PDB['Atomicnumberofseededimpurity'].isin([0, 7])) & (JET_PDB['FLAG:DEUTERIUM'] == 1.0) & (JET_PDB['FLAG:Kicks'] == 0.0) & (JET_PDB['FLAG:RMP'] == 0.0) & (JET_PDB['FLAG:pellets'] == 0.0)]\n",
    "\n",
    "dataset = full_dict['all_dict']['raw']['profiles'], full_dict['all_dict']['raw']['real_space_radii'], full_dict['all_dict']['raw']['pulse_time_ids'], full_dict['all_dict']['raw']['uncerts'], full_dict['all_dict']['raw']['controls']\n",
    "profiles, rmids, ids, uncerts, mps = dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77779e6c-48ec-47ed-8feb-b147a476596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_q95_with_qcly(mp_set, mu, var):\n",
    "    mp_set = de_standardize(mp_set, mu, var)\n",
    "    mu_0 = 1.25663706e-6 # magnetic constant\n",
    "    \n",
    "    mp_set[:, 0] = ((1 + 2*mp_set[:, 6]**2) / 2.0) * (2*mp_set[:, 9]*torch.pi*mp_set[:, 2]**2) / (mp_set[:, 1] * mp_set[:, 8] * mu_0)\n",
    "    mp_set = standardize(mp_set, mu, var)\n",
    "    return mp_set\n",
    "def extend_used_dict(pulse_num, dict_to_update, initialize=False, **kwargs): \n",
    "    if pulse_num not in dict_to_update.keys(): \n",
    "        initialize = True\n",
    "    for key in kwargs: \n",
    "        if initialize: \n",
    "            dict_to_update[pulse_num] = {}\n",
    "            dict_to_update[pulse_num][key] = []\n",
    "        dict_to_update[pulse_num][key].append(kwargs[key])\n",
    "    return dict_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6b2195-8801-49fc-9553-c7c190213237",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# boilerplate stuff\n",
    "import math\n",
    "from collections.abc import Iterable\n",
    "\n",
    "class RunningStats:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.old_m = 0\n",
    "        self.new_m = 0\n",
    "        self.old_s = 0\n",
    "        self.new_s = 0\n",
    "\n",
    "    def clear(self):\n",
    "        self.n = 0\n",
    "\n",
    "    def push(self, x):\n",
    "        if isinstance(x, Iterable):\n",
    "            for v in x:\n",
    "                self.push(v)\n",
    "            return\n",
    "\n",
    "        self.n += 1\n",
    "\n",
    "        if self.n == 1:\n",
    "            self.old_m = self.new_m = x\n",
    "            self.old_s = 0\n",
    "        else:\n",
    "            self.new_m = self.old_m + (x - self.old_m) / self.n\n",
    "            self.new_s = self.old_s + (x - self.old_m) * (x - self.new_m)\n",
    "\n",
    "            self.old_m = self.new_m\n",
    "            self.old_s = self.new_s\n",
    "\n",
    "    def mean(self):\n",
    "        return self.new_m if self.n else 0.0\n",
    "\n",
    "    def variance(self):\n",
    "        return self.new_s / (self.n - 1) if self.n > 1 else 0.0\n",
    "\n",
    "    def standard_deviation(self):\n",
    "        return math.sqrt(self.variance())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'n: {self.n}, mean: {self.mean()}, var: {self.variance()}, sd: {self.standard_deviation()}'\n",
    "    \n",
    "def standardize_signal(x, trim_zeros=True):\n",
    "    if trim_zeros:\n",
    "        x_in = np.trim_zeros(x)\n",
    "    else:\n",
    "        x_in = x\n",
    "    rs = RunningStats() # numpy.std goes to inf so do it by hand\n",
    "    rs.push(x_in)\n",
    "    return (x - rs.mean()) / rs.standard_deviation()\n",
    "\n",
    "def find_nearest(query, data, idx_must_be='any'):\n",
    "    if idx_must_be not in ['any', 'smaller', 'greater']:\n",
    "        raise ValueError('\"idx_must_be\" must be in [any, smaller, greater]')\n",
    "    # ASSUMED A SORTED ARRAY [INCREASING] #\n",
    "    # optionally, if query is inbetween elements, can pick which direction op\n",
    "    if query < data[0]:\n",
    "        if idx_must_be == 'smaller':\n",
    "            raise ValueError('Cannot return smaller value as \"query\" is smaller than any datapoint')\n",
    "        return 0\n",
    "    elif query > data[-1]:\n",
    "        if idx_must_be == 'greater':\n",
    "            raise ValueError('Cannot return greater value as \"query\" is greater than any datapoint')\n",
    "        return len(data)-1\n",
    "    else:\n",
    "        idx = np.searchsorted(data, query, side='left')\n",
    "        if data[idx] == query:\n",
    "            return idx\n",
    "        elif idx_must_be == 'any':\n",
    "            if abs(query - data[idx-1]) < abs(query - data[idx]):\n",
    "                return idx-1\n",
    "            else:\n",
    "                return idx\n",
    "        elif idx_must_be == 'greater':\n",
    "            return idx\n",
    "        elif idx_must_be == 'smaller':\n",
    "            return idx - 1\n",
    "        else:\n",
    "            raise ValueError('invalid argument for \"idx_must_be\"')\n",
    "            \n",
    "def get_nearest_weighted_idx(query, data, sort=True):\n",
    "    if sort:\n",
    "        unsorted = np.array(data)\n",
    "        sortidx = np.argsort(data)\n",
    "        sort_reverse = {i: sortidx[i] for i in range(len(data))}\n",
    "\n",
    "        data = np.array(data)[sortidx]\n",
    "    data = list(data)\n",
    "    if query in data: # exact match\n",
    "        idx = [(1, data.index(query))] # weight, index\n",
    "    elif query < data[0]: # before first\n",
    "        idx = [(1, 0)] \n",
    "    elif query > data[-1]: # after last\n",
    "        idx = [(1, len(data)-1)]\n",
    "    else:\n",
    "        # get nearest two elements\n",
    "        i_r = np.searchsorted(data, query)\n",
    "        i_l = i_r - 1\n",
    "        dist = data[i_r] - query + query - data[i_l]\n",
    "        idx = [(1-(data[i_r] - query)/dist, i_r), (1-(query - data[i_l])/dist, i_l)]\n",
    "\n",
    "    # convert back to unsorted idx\n",
    "    if sort:\n",
    "        idx = [(w, sort_reverse[i]) for (w, i) in idx]\n",
    "    return idx\n",
    "def pedestal_top(p, x, plot_result=False):\n",
    "    # standardize signal\n",
    "    p = standardize_signal(p, trim_zeros=True)\n",
    "\n",
    "    # interp signal to Nx=50\n",
    "    f_interp = interpolate.interp1d(x, p)\n",
    "    x_h = np.linspace(x[0], x[-1], 50)  # interpolate to 50 (evenly spaced) points\n",
    "    p_h = f_interp(x_h)\n",
    "\n",
    "    # smooth with savgol filter\n",
    "    p_s = savgol_filter(p_h, window_length=11, polyorder=3)\n",
    "\n",
    "    # get max gradient so we're in the pedestal\n",
    "    p_s_grad = np.gradient(p_s)\n",
    "    min_i = np.argmin(p_s_grad)\n",
    "\n",
    "\n",
    "    # search from pedestal region outward in 2nd derivatives\n",
    "    p_s_grad2 = np.gradient(p_s_grad)\n",
    "    p_s_grad2 = savgol_filter(p_s_grad2, window_length=11, polyorder=3)  # aggressively smooth as well\n",
    "    p_s_grad2 = standardize_signal(p_s_grad2, trim_zeros=True)\n",
    "    # standardize s.t. if we go >1 sd up/down, we stop searching\n",
    "\n",
    "    # go to the left from middle point\n",
    "    sd_cutoff = -.5\n",
    "    start_cut_early = False  # if we go < -1 for x'', mark as such, such that if we go > -1 again we stop looking\n",
    "    min_val = p_s_grad2[min_i]\n",
    "    top_i = min_i\n",
    "    for i in reversed(range(0, min_i)):\n",
    "        if p_s_grad2[i] < min_val:\n",
    "            min_val = p_s_grad2[i]\n",
    "            top_i = i\n",
    "            if min_val < sd_cutoff:\n",
    "                start_cut_early = True\n",
    "        elif start_cut_early and p_s_grad2[i] > sd_cutoff:\n",
    "            break\n",
    "\n",
    "    # found our x for top\n",
    "    # same procedure for bottom\n",
    "    sd_cutoff = .5\n",
    "    start_cut_early = False  # if we go > 1 for x'', etc.\n",
    "    max_val = p_s_grad2[min_i]\n",
    "    bottom_i = min_i\n",
    "\n",
    "    for i in range(min_i+1, x_h.shape[0]):\n",
    "        if p_s_grad2[i] > max_val:\n",
    "            max_val = p_s_grad2[i]\n",
    "            bottom_i = i\n",
    "            if max_val > sd_cutoff:\n",
    "                start_cut_early = True\n",
    "        elif start_cut_early and p_s_grad2[i] < sd_cutoff:\n",
    "            break\n",
    "\n",
    "    top, bottom = x_h[top_i], x_h[bottom_i]\n",
    "\n",
    "    if plot_result:\n",
    "        plt.plot(x_h, p_s)\n",
    "        plt.axvline(top, color='red', label='estimated top')\n",
    "        plt.axvline(bottom, color='blue', label='estimated bottom')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return top, bottom\n",
    "\n",
    "def find_separatrix(ne, te, x, plot_result=False):\n",
    "    top_x, bottom_x = pedestal_top(ne, x, plot_result=False)\n",
    "    \n",
    "    # initial estimate\n",
    "    x_sep = (1/4) * top_x + (3/4) * bottom_x\n",
    "    x_initial = x_sep\n",
    "    # scan area around x_sep\n",
    "    # first, interp to higher space\n",
    "    x_h = np.linspace(x[0], x[-1], 200)  # interpolate to 50 (evenly spaced) points\n",
    "    \n",
    "    f_ne_interp = interpolate.interp1d(x, ne)\n",
    "    ne_h = f_ne_interp(x_h)\n",
    "    ne_s = savgol_filter(ne_h, window_length=11, polyorder=3)\n",
    "    f_te_interp = interpolate.interp1d(x, te)\n",
    "    te_h = f_te_interp(x_h)\n",
    "    te_s = savgol_filter(te_h, window_length=11, polyorder=3)\n",
    "    \n",
    "    # closest x\n",
    "    s_i = find_nearest(x_sep, x_h)\n",
    "    # scan inwards/outwards\n",
    "    n_step = 10\n",
    "    te_target = 100 # 100 eV\n",
    "    te_distr = norm(te_target, 10)  # distribution to weigh new points against\n",
    "    weight_func = lambda x: x ** 0.05\n",
    "    \n",
    "    def te_sep():\n",
    "        idx = get_nearest_weighted_idx(x_sep, x_h, sort=False)\n",
    "        val_out = 0\n",
    "        for w, w_i in idx:\n",
    "            val_out += w * te_s[w_i]\n",
    "        return val_out\n",
    "    \n",
    "    for i in range(1, n_step+1):\n",
    "        i_left = s_i - i\n",
    "        i_right = s_i + i\n",
    "        if i_left < 0 or i_right > x_h.shape[0] - 1:\n",
    "            break  # reached outside the grid on 1 side\n",
    "        weight_pos = i / n_step  # [1/n, 1] adjustment based on how far the value is 'good'\n",
    "        pdf_l = te_distr.pdf(te_s[i_left])\n",
    "        adjust_l = weight_pos * pdf_l\n",
    "        adjust_l = weight_func(adjust_l)\n",
    "        if te_s[i_left] > te_target and te_sep() < te_target:  # on the left is above 100 --> we need to move left\n",
    "            x_sep = (x_sep + x_h[i_left] * adjust_l) / (1 + adjust_l)\n",
    "\n",
    "        pdf_r = te_distr.pdf(te_s[i_right])\n",
    "        adjust_r = weight_pos * pdf_r\n",
    "        adjust_r = weight_func(adjust_r)\n",
    "        if te_s[i_right] < te_target and te_sep() > te_target:  # on the right is below 100 --> we need to move right\n",
    "            x_sep = (x_sep + x_h[i_right] * adjust_r) / (1 + adjust_r)\n",
    "    \n",
    "    # closest_x_index = np.argmin(abs(x_h - x_sep))\n",
    "    # from_fit = x_h[closest_x_index], te_h[closest_x_index], ne_h[closest_x_index]\n",
    "    interp_idx_l, interp_idx_r = get_idx_for_linear_interp(x_sep, x, te)\n",
    "    weights_r, weights_l = get_weights(te, interp_idx_l, interp_idx_r)\n",
    "    tesep_estimation = weights_l*te[interp_idx_l] + weights_r*te[interp_idx_r]\n",
    "    nesep_estimation = weights_l*ne[interp_idx_l] + weights_r*ne[interp_idx_r]\n",
    "    rsep_estimation = weights_l*x[interp_idx_l] + weights_r*x[interp_idx_r]\n",
    "    if nesep_estimation < 0: \n",
    "        plot_result = True\n",
    "    if plot_result:\n",
    "        fig, axs=plt.subplots(1, 2, figsize=(6,3))\n",
    "        axs[0].plot(x_h, te_s)\n",
    "        axs[0].scatter(x, te)\n",
    "        axs[0].scatter(x[interp_idx_l], te[interp_idx_l], color='orange')\n",
    "        axs[0].scatter(x[interp_idx_r], te[interp_idx_r], color='orange')\n",
    "        \n",
    "        axs[0].axvline(x_initial, color='red', label='initial estimate')\n",
    "        axs[0].axvline(x_sep, color='green', label='final estimate')\n",
    "        axs[0].axhline(100, color='black', label='100eV')\n",
    "        axs[0].scatter(rsep_estimation, tesep_estimation, color='black',marker='*', s=250)\n",
    "\n",
    "        axs[1].plot(x_h, ne_s)\n",
    "        axs[1].scatter(x, ne)\n",
    "        axs[1].scatter(rsep_estimation, nesep_estimation, color='black',marker='*', s=250)\n",
    "        axs[1].axvline(x_initial, color='red', label='initial prior')\n",
    "        axs[1].axvline(x_sep, color='green', label='final prior')\n",
    "        axs[1].axvline(rsep_estimation, color='black', ls='--', label='final est')\n",
    "        axs[1].scatter(x[interp_idx_l], ne[interp_idx_l], color='orange')\n",
    "        axs[1].scatter(x[interp_idx_r], ne[interp_idx_r], color='orange')\n",
    "        axs[1].legend()\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return tesep_estimation, nesep_estimation, rsep_estimation\n",
    "\n",
    "def get_idx_for_linear_interp(x_sep, x, te): \n",
    "    closest_point = np.argmin(abs(x - x_sep))\n",
    "    if (te < 100).sum() == 0: \n",
    "        closest_point = np.argmin(abs(te - 100))\n",
    "        idx_r = closest_point\n",
    "        idx_l = idx_r - 1\n",
    "   \n",
    "    elif te[closest_point] < 100: \n",
    "        idx_r = closest_point\n",
    "        idx_l = closest_point - 1\n",
    "        while te[idx_l] < 100: \n",
    "            idx_r = idx_l\n",
    "            idx_l = idx_l - 1\n",
    "    else: \n",
    "        idx_r = closest_point + 1\n",
    "        idx_l = closest_point\n",
    "        while te[idx_r] > 100:\n",
    "            idx_l = idx_r\n",
    "            idx_r = idx_l + 1\n",
    "            \n",
    "    return idx_l, idx_r \n",
    "def get_weights(te, idx_l, idx_r, query=100): \n",
    "    dist = te[idx_r] - query + query - te[idx_l]\n",
    "    weights = (1-(te[idx_r] - query)/dist, 1-(query - te[idx_l])/dist) \n",
    "    return weights\n",
    "def find_elm_percent(pulse_num, time): \n",
    "    try: \n",
    "        pulse_elm_timings_frass = np.array(JET_ELM_TIMINGS[pulse_num])\n",
    "    except KeyError as e:\n",
    "        return np.nan\n",
    "    diff = pulse_elm_timings_frass - time\n",
    "    try:\n",
    "        time_last_elm = pulse_elm_timings_frass[diff < 0][-1]\n",
    "        time_next_elm = pulse_elm_timings_frass[diff > 0][0]\n",
    "        elm_percent = (time - time_last_elm) / (time_next_elm - time_last_elm)\n",
    "    except IndexError as e:\n",
    "        elm_percent = np.nan\n",
    "    return elm_percent\n",
    "\n",
    "def get_lorenzo_pred(pulse_num, pulse_time): \n",
    "    JPDB_pulse = PULSE_DF_SANDBOX[PULSE_DF_SANDBOX['shot'] == pulse_num]\n",
    "    if len(JPDB_pulse) > 1: \n",
    "        print(JPDB_pulse[['t1', 't2']])\n",
    "        for local_pulse in JPDB_pulse: \n",
    "            t1, t2 = local_pulse[['t1', 't2']]\n",
    "            if t1 < pulse_time and t2 > pulse_time: \n",
    "                nesep_exp, nesep_lor = local_pulse[['neseparatrixfromexpdata10^19(m^-3)', 'neseparatrixfromfit10^19(m^-3)']].values[0]\n",
    "            else: \n",
    "                continue\n",
    "    else: \n",
    "        index_to_take_from = 0\n",
    "        nesep_exp, nesep_lor = JPDB_pulse[['neseparatrixfromexpdata10^19(m^-3)', 'neseparatrixfromfit10^19(m^-3)']].values[0]\n",
    "        \n",
    "    return nesep_exp, nesep_lor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d12511-5ffc-4cba-bc9b-22fa79f03aa9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_dataset(all_dataset): \n",
    "    \"\"\"\n",
    "    This organizes all the data by pulse.  \n",
    "    We can then split by different pulses for training/val/test later. \n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    the whole dataset! Found earlier, but a \n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    used_dict: a dictionary containing all data by pulse number: \n",
    "        used_dict[123456]['profiles'] corresponds to the profiles of pulse\n",
    "        with keys: 'profiles', 'neseps', 'ids', 'mps', 'elm_perc', 'lor_val', 'mask'\n",
    "    \"\"\"\n",
    "    \n",
    "    profiles, rmids, ids, uncerts, mps = all_dataset \n",
    "    iterator = tqdm(range(len(profiles)))\n",
    "    used_dict = {}\n",
    "    \n",
    "    for idx in iterator: \n",
    "        name = ids[idx]\n",
    "        pulse_num, time = name.split('/')\n",
    "        elm_perc = find_elm_percent(int(pulse_num), float(time))\n",
    "        if np.isnan(elm_perc): \n",
    "            continue\n",
    "        else: \n",
    "            # used_elm_percents.append(elm_perc)\n",
    "            pass\n",
    "        _, lor_val = get_lorenzo_pred(int(pulse_num), time)\n",
    "            \n",
    "        original = profiles[idx][0], profiles[idx][1],uncerts[idx][0], uncerts[idx][1], rmids[idx]\n",
    "        find_elm_percent(pulse_num, time)\n",
    "        ne, te, dne, dte, x = original\n",
    "        new_ne, new_te =  np.minimum.accumulate(ne), np.minimum.accumulate(te)\n",
    "        keep_idx = np.where(new_te == te)\n",
    "\n",
    "        ne, te, dne, dte, x = new_ne[keep_idx], new_te[keep_idx], dne[keep_idx], dte[keep_idx], x[keep_idx]\n",
    "        logical_bool_mask = np.logical_and(ne > 0, te > 0) # check that nothing is below zero, i mean what the fuck\n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dte > 0) # Also here\n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dne > 0) # Also here \n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dte < 3000) # Don't want tesep values that are ridonklus\n",
    "        ne, te, dne, dte, x = ne[logical_bool_mask], te[logical_bool_mask], dne[logical_bool_mask], dte[logical_bool_mask], x[logical_bool_mask]\n",
    "        if (te < 100).sum() == 0:\n",
    "            continue\n",
    "        else: \n",
    "            try: \n",
    "                estimations = find_separatrix(ne, te, x, plot_result=False)\n",
    "            except IndexError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            else: \n",
    "                used_dict = extend_used_dict(pulse_num, used_dict, profiles=profiles[idx], neseps=estimations[1], ids=name, mps=mps[idx], elm_perc=elm_perc, lor_val=lor_val, rmids=rmids[idx], rseps=estimations[-1])\n",
    "    cleaned_dict = clean_used_dict(used_dict)\n",
    "    return cleaned_dict     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27da5dc7-9127-42ed-9d0a-2e57aa28711b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extend_used_dict(pulse_num, dict_to_update, initialize=False, **kwargs): \n",
    "    if pulse_num not in dict_to_update.keys(): \n",
    "        initialize = True\n",
    "        dict_to_update[pulse_num] = {}\n",
    "    for key, item in kwargs.items(): \n",
    "        if initialize: \n",
    "            dict_to_update[pulse_num][key] = []\n",
    "        dict_to_update[pulse_num][key].append(item)\n",
    "    return dict_to_update\n",
    "\n",
    "def clean_used_dict(dict_to_update):\n",
    "    updated_dict = dict_to_update.copy()\n",
    "    for pulse_num, pulse_items in updated_dict.items(): \n",
    "        # Find a bool array list of indexes that should be kept. \n",
    "        nesep_array = np.array(pulse_items['neseps'])\n",
    "        nesep_mean, nesep_std = nesep_array.mean(), nesep_array.std()\n",
    "        slices_to_keep = abs(nesep_array - nesep_mean) < nesep_std\n",
    "        for key, item in pulse_items.items():\n",
    "            updated_dict[pulse_num][key] = [it for n, it in enumerate(item) if slices_to_keep[n]] # np.array(item)[slices_to_keep]\n",
    "    return updated_dict\n",
    "def get_pulse_train_val_split(sandbox_dict): \n",
    "    \"\"\"\n",
    "    Takes the pulse dict gathered above and returns the train-val-test split of the pulses \n",
    "    \"\"\"\n",
    "    train_size = 0.85\n",
    "    val_size = 0.2\n",
    "    pulse_nums = list(sandbox_dict.keys())\n",
    "    \n",
    "    k_train = int(len(pulse_nums) * train_size)\n",
    "    \n",
    "    indicies = random.sample(range(len(pulse_nums)), k_train)\n",
    "    \n",
    "    train_val_pulses = [pulse_nums[i] for i in indicies]\n",
    "    test_pulses = list(set(pulse_nums) - set(train_val_pulses))\n",
    "    \n",
    "    k_val = int(len(train_val_pulses) * val_size)\n",
    "    indicies = random.sample(range(len(train_val_pulses)), k_val)\n",
    "    \n",
    "    val_pulses = [train_val_pulses[i] for i in indicies]\n",
    "    \n",
    "    \n",
    "    train_pulses = list(set(train_val_pulses) - set(val_pulses))\n",
    "        \n",
    "    assert len(pulse_nums) == (len(train_pulses) + len(val_pulses) + len(test_pulses))\n",
    "    return train_pulses, val_pulses, test_pulses\n",
    "\n",
    "def make_dataset(all_dataset):\n",
    "    profiles, rmids, ids, uncerts, mps = all_dataset \n",
    "    iterator = tqdm(range(len(profiles)))\n",
    "    used_dict = {}\n",
    "    \n",
    "    for idx in iterator: \n",
    "        name = ids[idx]\n",
    "        pulse_num, time = name.split('/')\n",
    "        elm_perc = find_elm_percent(int(pulse_num), float(time))\n",
    "        if np.isnan(elm_perc): \n",
    "            continue\n",
    "        else: \n",
    "            # used_elm_percents.append(elm_perc)\n",
    "            pass\n",
    "        _, lor_val = get_lorenzo_pred(int(pulse_num), time)\n",
    "            \n",
    "        original = profiles[idx][0], profiles[idx][1],uncerts[idx][0], uncerts[idx][1], rmids[idx]\n",
    "        find_elm_percent(pulse_num, time)\n",
    "        ne, te, dne, dte, x = original\n",
    "        new_ne, new_te =  np.minimum.accumulate(ne), np.minimum.accumulate(te)\n",
    "        keep_idx = np.where(new_te == te)\n",
    "\n",
    "        ne, te, dne, dte, x = new_ne[keep_idx], new_te[keep_idx], dne[keep_idx], dte[keep_idx], x[keep_idx]\n",
    "        logical_bool_mask = np.logical_and(ne > 0, te > 0) # check that nothing is below zero, i mean what the fuck\n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dte > 0) # Also here\n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dne > 0) # Also here \n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dte < 3000) # Don't want tesep values that are ridonklus\n",
    "        ne, te, dne, dte, x = ne[logical_bool_mask], te[logical_bool_mask], dne[logical_bool_mask], dte[logical_bool_mask], x[logical_bool_mask]\n",
    "        if (te < 100).sum() == 0:\n",
    "            continue\n",
    "        else: \n",
    "            try: \n",
    "                estimations = find_separatrix(ne, te, x, plot_result=False)\n",
    "            except IndexError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            else: \n",
    "                used_dict = extend_used_dict(pulse_num, used_dict, profiles=profiles[idx], neseps=estimations[1], ids=name, mps=mps[idx], elm_perc=elm_perc, lor_val=lor_val, rmids=rmids[idx], masks=logical_bool_mask, rseps=estimations[-1])\n",
    "    cleaned_dict = clean_used_dict(used_dict)\n",
    "    return cleaned_dict\n",
    "def make_train_val_test_sets(sandbox_dict, train_pulses, val_pulses, test_pulses): \n",
    "    train_dict = make_subset(sandbox_dict, train_pulses)\n",
    "    val_dict = make_subset(sandbox_dict, val_pulses)\n",
    "    test_dict = make_subset(sandbox_dict, test_pulses)\n",
    "    return {'train': train_dict, 'val': val_dict, 'test': test_dict}       \n",
    "\n",
    "def pad_lists(list_to_pad, key, pad_length=19, **kwargs): \n",
    "    if key == 'profiles': \n",
    "        padded_profiles = np.zeros((len(list_to_pad), 2, pad_length))\n",
    "        for n, prof in enumerate(list_to_pad): \n",
    "            num_missing = pad_length - len(prof[0])\n",
    "            padded_prof = np.pad(prof, ((0,0), (num_missing,0)), mode='edge')\n",
    "            padded_profiles[n] = padded_prof\n",
    "        return padded_profiles\n",
    "    elif key == 'rmids': \n",
    "        padded_radii =  np.zeros((len(list_to_pad), 19))\n",
    "        for n, rmid in enumerate(list_to_pad): \n",
    "            num_missing = pad_length - len(rmid)\n",
    "            padded_rmid = np.pad(rmid, (num_missing,0), mode='constant') \n",
    "            padded_radii[n] = padded_rmid\n",
    "        return padded_radii\n",
    "    elif key == 'masks': \n",
    "        padded_masks = np.zeros((len(list_to_pad), 19)) \n",
    "        for n, mask in enumerate(list_to_pad): \n",
    "            num_missing = pad_length - len(mask)\n",
    "            padded_mask = np.pad(mask, (num_missing, 0), mode='constant', constant_values=False)\n",
    "            padded_masks[n] = padded_mask\n",
    "        return padded_masks\n",
    "    \n",
    "def make_subset(sandbox_dict, pulse_list): \n",
    "    \"\"\"\n",
    "    Makes a subset (train-val-test) dictionary from the dictionary of pulses given the pulses to use. \n",
    "    \n",
    "    Returns\n",
    "    ======\n",
    "    subset_dict: \n",
    "        keys: 'profiles', 'neseps', 'ids', 'mps', 'elm_perc', 'lor_val', 'rmids'\n",
    "    \n",
    "    \"\"\"\n",
    "    subset_dict = {}\n",
    "    for pulse_num in pulse_list: \n",
    "        pulse_dict = sandbox_dict[pulse_num]\n",
    "        for key, item in pulse_dict.items(): \n",
    "            if key not in subset_dict.keys(): \n",
    "                subset_dict[key] = []\n",
    "            if key in ['profiles', 'rmids', 'masks']: \n",
    "                padded_item = pad_lists(item, key)\n",
    "            else: \n",
    "                padded_item = np.array(item)\n",
    "            subset_dict[key].append(padded_item)\n",
    "    for key in subset_dict.keys(): \n",
    "        if key in ['profiles', 'rmids', 'masks', 'mps']: \n",
    "            subset_dict[key] = np.vstack(subset_dict[key])\n",
    "        else: \n",
    "            subset_dict[key] = np.concatenate(subset_dict[key])\n",
    "    return subset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65755f4b-7eff-4b4f-b123-9c7b75e884d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4454e4d1be4ca4b7e8b81fe81a334a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_dict = make_dataset(dataset)\n",
    "train_pulses, val_pulses, test_pulses = get_pulse_train_val_split(cleaned_dict)\n",
    "vae_ready_dict = make_train_val_test_sets(cleaned_dict, train_pulses, val_pulses, test_pulses)\n",
    "# has shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "815442c5-86cf-4169-bb36-f99b146b8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../moxie/data/processed/cleaned_pulse_dict_230522.pickle', 'wb') as file: \n",
    "    pickle.dump(cleaned_dict, file)\n",
    "with open('../../../moxie/data/processed/cleaned_ml_ready_dict_230522.pickle', 'wb') as file: \n",
    "    pickle.dump(vae_ready_dict, file)\n",
    "    \n",
    "# To use this for example, you need ot then split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "655b16c6-de31-4b52-8df9-394a9d3cffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['profiles', 'neseps', 'ids', 'mps', 'elm_perc', 'lor_val', 'rmids', 'masks', 'rseps'])\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_dict['81794'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b18efcd-779f-4c0e-9103-102f69625617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11157, 2, 19) (11157, 13) (11157,)\n"
     ]
    }
   ],
   "source": [
    "print(vae_ready_dict['train']['profiles'].shape, vae_ready_dict['train']['mps'].shape, vae_ready_dict['train']['elm_perc'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e1ea2a9-a2db-4a0d-8e98-c68567316cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['profiles', 'neseps', 'ids', 'mps', 'elm_perc', 'lor_val', 'rmids', 'masks']),\n",
       " dict_keys(['train', 'val', 'test']))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_ready_dict['train'].keys(), vae_ready_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b9451df-306e-4252-b068-edd0d7a3c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_train, elm_train, mps_train = vae_ready_dict['train']['profiles'],vae_ready_dict['train']['elm_perc'], vae_ready_dict['train']['mps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11eb86ca-3b87-4fdf-8c2d-aab31591f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "846b0fda-8cc8-4500-9001-4c0a258804fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, ELM_train = torch.from_numpy(profiles_train),torch.from_numpy(elm_train), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff548bbe-6338-4c8b-af24-4b22522b3f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11157, 2, 19]), torch.Size([11157]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, ELM_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "793e8fbb-52b5-4cbb-9e51-5ab9195c4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([11157, 1, 19]), torch.Size([11157]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(ELM_train.unsqueeze(1), 19, 1).unsqueeze(1).shape, ELM_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8753604-b7df-426d-9e09-28189b127b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "ELM_train_ready = torch.repeat_interleave(ELM_train.unsqueeze(1), 19, 1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29713b90-b2c4-475e-8df3-c51e63c89999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2426, 0.2426, 0.2426, 0.2426, 0.2426, 0.2426, 0.2426, 0.2426, 0.2426,\n",
       "        0.2426, 0.2426, 0.2426, 0.2426, 0.2426, 0.2426, 0.2426, 0.2426, 0.2426,\n",
       "        0.2426], dtype=torch.float64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat((X_train, ELM_train_ready), 1)[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "54411ffa-0f55-4d09-8787-0aa4331b2e65",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_77354/244733043.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "X_train[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4614481-8079-4598-a362-d5e9055c30cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11157, 1)\n"
     ]
    }
   ],
   "source": [
    "print(mps_train[:, -1:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ef6f974-01a4-4f97-895a-142c0232af55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0188041e+22, 1.0038003e+22, 9.9620153e+21, ..., 1.2249083e+22,\n",
       "       1.2246535e+22, 1.2242428e+22], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_train[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8d5d4e3e-f874-43da-ad0b-b08503029763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.1123648e+00,  2.9231858e+00,  9.2052871e-01, ...,\n",
       "        -2.2657740e+00,  1.9579650e+07,  0.0000000e+00],\n",
       "       [ 4.1831694e+00,  2.9166126e+00,  9.2619830e-01, ...,\n",
       "        -2.2654254e+00,  1.9127518e+07,  0.0000000e+00],\n",
       "       [ 4.1876640e+00,  2.9145408e+00,  9.2521960e-01, ...,\n",
       "        -2.2652464e+00,  1.9769564e+07,  0.0000000e+00],\n",
       "       ...,\n",
       "       [ 3.1329529e+00,  2.9051933e+00,  9.2382020e-01, ...,\n",
       "        -2.1113687e+00,  7.4103455e+06,  0.0000000e+00],\n",
       "       [ 3.1186874e+00,  2.9084048e+00,  9.2181474e-01, ...,\n",
       "        -2.1115985e+00,  7.4067820e+06,  0.0000000e+00],\n",
       "       [ 3.1158543e+00,  2.9091759e+00,  9.2034882e-01, ...,\n",
       "        -2.1115129e+00,  7.7074730e+06,  0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_train[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b18f253-b2dd-48f7-869a-ee9d7e525809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
