{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90fafa90-d750-4449-a722-67504a3b6e1b",
   "metadata": {},
   "source": [
    "# Creating the dataset\n",
    "\n",
    "We are using data from JET. \n",
    "\n",
    "### Starting data \n",
    "\n",
    "- HRTS profiles \n",
    "    - Electron density and temperature, $n_e$, $T_e$\n",
    "    - HRTS Line of Sight coordinates, $R_{HRTS}$\n",
    "- Machine parameters\n",
    "    - $R$ [m], plasma major radius\n",
    "    - $a$ [m], plasma minor radius \n",
    "    - $V_P$ [m$^3$], total plasma volume enclosed by LCFS\n",
    "    - $\\delta_u,\\delta_l$ [-], upper and lower triangularities  \n",
    "    - $\\kappa$ [-], elongation, or ratio of height to width of plasma\n",
    "    - $P_{OHM}$ [W], ohmic power of the plasma\n",
    "    - $I_P$ [A], total plasma current enclosed by LCFS \n",
    "    - $B_T$ [T], total toroidal magnetic field\n",
    "    - $P_{NBI}$ [W], total neutral beam injected power into the plasma\n",
    "    - $P_{ICRH}$ [W], total ion cyclotron resonance heating imposed onto plasma\n",
    "    - $\\Gamma$ [e/s], fuelling rate of main isotope (aka gas puff)\n",
    "    - $q_{cycl}$ [-], safety factor, calculated from $q_{cycl} = \\frac{(1+2\\kappa^2)}{2}\\frac{2B_T\\pi a^2}{RI_P\\mu_0}$\n",
    "    \n",
    "#### Profile Information \n",
    "\n",
    "- Pulses/time windows found in JET pedestal database\n",
    "- Subset of pulses: \n",
    "    - ILW\n",
    "    - HRTS Validated \n",
    "    - Pulses with either no seeding or with nitrogen seeding\n",
    "    - Deuterium fuelling\n",
    "    - No kicks, rmps, or pellets\n",
    "    \n",
    "\n",
    "\n",
    "After filtering above: \n",
    "- Total number of pulses & time slices\n",
    "    - 1248\n",
    "   \n",
    "### Further slice filtering \n",
    "\n",
    "#### ELM Percents\n",
    "\n",
    "Slices which an ELM percentage could be calculated, using ELM timings from JET pedestal database\n",
    "\n",
    "\n",
    "After filtering: \n",
    "- Total number of slices from 608 pulses: \n",
    "    - 23499\n",
    "    \n",
    "#### $n_{e, sep}$\n",
    "\n",
    "- TODO: Describe process of gathering $n_{e, sep}$\n",
    "- Calculation is not perfect, so we remove slices that have an $n_{e, sep}$ approximation that falls outside of 1 standard deviation of the mean $n_{e, sep}$ for the pulse. \n",
    "\n",
    "- Total number of slices: \n",
    "    - 16111\n",
    "    \n",
    "## Profile padding \n",
    "\n",
    "The maximum length of the profiles is 19, so all will be padded to that value. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a15d52-c539-4a9f-b6f5-3592a4e5d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy import interpolate\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import norm\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77779e6c-48ec-47ed-8feb-b147a476596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_q95_with_qcly(mp_set, mu, var):\n",
    "    mp_set = de_standardize(mp_set, mu, var)\n",
    "    mu_0 = 1.25663706e-6 # magnetic constant\n",
    "    \n",
    "    mp_set[:, 0] = ((1 + 2*mp_set[:, 6]**2) / 2.0) * (2*mp_set[:, 9]*torch.pi*mp_set[:, 2]**2) / (mp_set[:, 1] * mp_set[:, 8] * mu_0)\n",
    "    mp_set = standardize(mp_set, mu, var)\n",
    "    return mp_set\n",
    "def extend_used_dict(pulse_num, dict_to_update, initialize=False, **kwargs): \n",
    "    if pulse_num not in dict_to_update.keys(): \n",
    "        initialize = True\n",
    "    for key in kwargs: \n",
    "        if initialize: \n",
    "            dict_to_update[pulse_num] = {}\n",
    "            dict_to_update[pulse_num][key] = []\n",
    "        dict_to_update[pulse_num][key].append(kwargs[key])\n",
    "    return dict_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d12511-5ffc-4cba-bc9b-22fa79f03aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(all_dataset): \n",
    "    \"\"\"\n",
    "    This organizes all the data by pulse.  \n",
    "    We can then split by different pulses for training/val/test later. \n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    the whole dataset! Found earlier, but a \n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    \n",
    "    used_dict: a dictionary containing all data by pulse number: \n",
    "        used_dict[123456]['profiles'] corresponds to the profiles of pulse\n",
    "        with keys: 'profiles', 'neseps', 'ids', 'mps', 'elm_perc', 'lor_val', 'mask'\n",
    "    \"\"\"\n",
    "    profiles, rmids, ids, uncerts, mps = all_dataset \n",
    "    iterator = tqdm(range(len(profiles)))\n",
    "    \n",
    "    # Organize by pulse number\n",
    "    used_dict = {}\n",
    "    # e.g., \n",
    "    for idx in iterator: \n",
    "        name = ids[idx]\n",
    "        pulse_num, time = name.split('/')\n",
    "        elm_perc = find_elm_percent(int(pulse_num), float(time))\n",
    "        # IF THERE IS NO ELM PERCENTAGE, THEN GO TO THE NEXT SLICE\n",
    "        if np.isnan(elm_perc): \n",
    "            continue \n",
    "        else: \n",
    "            pass\n",
    "        original = profiles[idx][0], profiles[idx][1],uncerts[idx][0], uncerts[idx][1], rmids[idx], mps[idx]\n",
    "        \n",
    "        ne, te, dne, dte, x, mp = original\n",
    "        # First we make everything monotonic deacreasing \n",
    "        new_ne, new_te =  np.minimum.accumulate(ne), np.minimum.accumulate(te)\n",
    "        keep_idx = np.where(new_te == te)\n",
    "        ne, te, dne, dte, x = new_ne[keep_idx], new_te[keep_idx], dne[keep_idx], dte[keep_idx], x[keep_idx]\n",
    "        # Get mask for the VAE learning process and finding nesep! \n",
    "        logical_bool_mask = np.logical_and(ne > 0, te > 0) # check that nothing is below zero, i mean what the fuck\n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dte > 0) # Also here\n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dne > 0) # Also here \n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dte < 3000) # Don't want tesep values that are ridonklus\n",
    "        # check if we still have any values below te < 100, if not then we can not reliably find nesep! \n",
    "        ne, te, dne, dte, x = ne[logical_bool_mask], te[logical_bool_mask], dne[logical_bool_mask], dte[logical_bool_mask], x[logical_bool_mask]\n",
    "        if (te < 100).sum() == 0:\n",
    "            continue\n",
    "        else: \n",
    "            try: \n",
    "                estimations = find_separatrix(ne, te, x, plot_result=False)\n",
    "            except IndexError as e: \n",
    "                print(e)\n",
    "                continue \n",
    "            else: \n",
    "                used_dict = extend_used_dict(pulse_num, used_dict, profiles=profiles[idx], neseps=estimations[1], ids=name, mps=mps[idx], elm_perc=elm_perc, lor_val=lor_val, mask=logical_bool_mask)        \n",
    "                \n",
    "    # Do something with the used dict! \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da5dc7-9127-42ed-9d0a-2e57aa28711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pulse_train_val_split(sandbox_dict): \n",
    "    \"\"\"\n",
    "    Takes the pulse dict gathered above and returns the train-val-test split of the pulses \n",
    "    \"\"\"\n",
    "    train_size = 0.85\n",
    "    val_size = 0.2\n",
    "    pulse_nums = list(sandbox_dict.keys())\n",
    "    \n",
    "    k_train = int(len(pulse_nums) * train_size)\n",
    "    \n",
    "    indicies = random.sample(range(len(pulse_nums)), k_train)\n",
    "    \n",
    "    train_val_pulses = [pulse_nums[i] for i in indicies]\n",
    "    test_pulses = list(set(pulse_nums) - set(train_val_pulses))\n",
    "    \n",
    "    k_val = int(len(train_val_pulses) * val_size)\n",
    "    indicies = random.sample(range(len(train_val_pulses)), k_val)\n",
    "    \n",
    "    val_pulses = [train_val_pulses[i] for i in indicies]\n",
    "    \n",
    "    \n",
    "    train_pulses = list(set(train_val_pulses) - set(val_pulses))\n",
    "        \n",
    "    assert len(pulse_nums) == (len(train_pulses) + len(val_pulses) + len(test_pulses))\n",
    "    return train_pulses, val_pulses, test_pulses\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
