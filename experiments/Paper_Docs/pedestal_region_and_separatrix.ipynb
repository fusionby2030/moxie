{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d80843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import norm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c46b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate stuff\n",
    "import math\n",
    "from collections.abc import Iterable\n",
    "\n",
    "class RunningStats:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.old_m = 0\n",
    "        self.new_m = 0\n",
    "        self.old_s = 0\n",
    "        self.new_s = 0\n",
    "\n",
    "    def clear(self):\n",
    "        self.n = 0\n",
    "\n",
    "    def push(self, x):\n",
    "        if isinstance(x, Iterable):\n",
    "            for v in x:\n",
    "                self.push(v)\n",
    "            return\n",
    "\n",
    "        self.n += 1\n",
    "\n",
    "        if self.n == 1:\n",
    "            self.old_m = self.new_m = x\n",
    "            self.old_s = 0\n",
    "        else:\n",
    "            self.new_m = self.old_m + (x - self.old_m) / self.n\n",
    "            self.new_s = self.old_s + (x - self.old_m) * (x - self.new_m)\n",
    "\n",
    "            self.old_m = self.new_m\n",
    "            self.old_s = self.new_s\n",
    "\n",
    "    def mean(self):\n",
    "        return self.new_m if self.n else 0.0\n",
    "\n",
    "    def variance(self):\n",
    "        return self.new_s / (self.n - 1) if self.n > 1 else 0.0\n",
    "\n",
    "    def standard_deviation(self):\n",
    "        return math.sqrt(self.variance())\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'n: {self.n}, mean: {self.mean()}, var: {self.variance()}, sd: {self.standard_deviation()}'\n",
    "    \n",
    "def standardize_signal(x, trim_zeros=True):\n",
    "    if trim_zeros:\n",
    "        x_in = np.trim_zeros(x)\n",
    "    else:\n",
    "        x_in = x\n",
    "    rs = RunningStats() # numpy.std goes to inf so do it by hand\n",
    "    rs.push(x_in)\n",
    "    return (x - rs.mean()) / rs.standard_deviation()\n",
    "\n",
    "def find_nearest(query, data, idx_must_be='any'):\n",
    "    if idx_must_be not in ['any', 'smaller', 'greater']:\n",
    "        raise ValueError('\"idx_must_be\" must be in [any, smaller, greater]')\n",
    "    # ASSUMED A SORTED ARRAY [INCREASING] #\n",
    "    # optionally, if query is inbetween elements, can pick which direction op\n",
    "    if query < data[0]:\n",
    "        if idx_must_be == 'smaller':\n",
    "            raise ValueError('Cannot return smaller value as \"query\" is smaller than any datapoint')\n",
    "        return 0\n",
    "    elif query > data[-1]:\n",
    "        if idx_must_be == 'greater':\n",
    "            raise ValueError('Cannot return greater value as \"query\" is greater than any datapoint')\n",
    "        return len(data)-1\n",
    "    else:\n",
    "        idx = np.searchsorted(data, query, side='left')\n",
    "        if data[idx] == query:\n",
    "            return idx\n",
    "        elif idx_must_be == 'any':\n",
    "            if abs(query - data[idx-1]) < abs(query - data[idx]):\n",
    "                return idx-1\n",
    "            else:\n",
    "                return idx\n",
    "        elif idx_must_be == 'greater':\n",
    "            return idx\n",
    "        elif idx_must_be == 'smaller':\n",
    "            return idx - 1\n",
    "        else:\n",
    "            raise ValueError('invalid argument for \"idx_must_be\"')\n",
    "            \n",
    "def get_nearest_weighted_idx(query, data, sort=True):\n",
    "    if sort:\n",
    "        unsorted = np.array(data)\n",
    "        sortidx = np.argsort(data)\n",
    "        sort_reverse = {i: sortidx[i] for i in range(len(data))}\n",
    "\n",
    "        data = np.array(data)[sortidx]\n",
    "    data = list(data)\n",
    "    if query in data: # exact match\n",
    "        idx = [(1, data.index(query))] # weight, index\n",
    "    elif query < data[0]: # before first\n",
    "        idx = [(1, 0)] \n",
    "    elif query > data[-1]: # after last\n",
    "        idx = [(1, len(data)-1)]\n",
    "    else:\n",
    "        # get nearest two elements\n",
    "        i_r = np.searchsorted(data, query)\n",
    "        i_l = i_r - 1\n",
    "        dist = data[i_r] - query + query - data[i_l]\n",
    "        idx = [(1-(data[i_r] - query)/dist, i_r), (1-(query - data[i_l])/dist, i_l)]\n",
    "\n",
    "    # convert back to unsorted idx\n",
    "    if sort:\n",
    "        idx = [(w, sort_reverse[i]) for (w, i) in idx]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "827b5359-322c-4746-9bcb-1bb64d2729de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(full_dict['all_dict']['raw'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b076a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_choice='SANDBOX_NO_VARIATIONS'\n",
    "file_loc='../../../moxie/data/processed/pedestal_profiles_ML_READY_ak_5052022_uncerts_mask.pickle'\n",
    "with open(file_loc, 'rb') as file:\n",
    "    massive_dict = pickle.load(file)\n",
    "full_dict = massive_dict[dataset_choice]\n",
    "massive_dict = {}\n",
    "with open('../../data/raw/new_elm_timings_catch.pickle', 'rb') as file: \n",
    "     JET_ELM_TIMINGS = pickle.load(file) \n",
    "\n",
    "dataset = full_dict['all_dict']['raw']['profiles'], full_dict['all_dict']['raw']['real_space_radii'], full_dict['all_dict']['raw']['pulse_time_ids'], full_dict['all_dict']['raw']['uncerts'], full_dict['all_dict']['raw']['controls']\n",
    "profiles, rmids, ids, uncerts, mps = dataset\n",
    "JET_PDB = pd.read_csv('../../../moxie/data/processed/jet-pedestal-database.csv')\n",
    "PULSE_DF_SANDBOX = JET_PDB[(JET_PDB['FLAG:HRTSdatavalidated'] > 0) & (JET_PDB['shot'] > 80000) & (JET_PDB['Atomicnumberofseededimpurity'].isin([0, 7])) & (JET_PDB['FLAG:DEUTERIUM'] == 1.0) & (JET_PDB['FLAG:Kicks'] == 0.0) & (JET_PDB['FLAG:RMP'] == 0.0) & (JET_PDB['FLAG:pellets'] == 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41ed8493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pedestal_top(p, x, plot_result=False):\n",
    "    # standardize signal\n",
    "    p = standardize_signal(p, trim_zeros=True)\n",
    "\n",
    "    # interp signal to Nx=50\n",
    "    f_interp = interpolate.interp1d(x, p)\n",
    "    x_h = np.linspace(x[0], x[-1], 50)  # interpolate to 50 (evenly spaced) points\n",
    "    p_h = f_interp(x_h)\n",
    "\n",
    "    # smooth with savgol filter\n",
    "    p_s = savgol_filter(p_h, window_length=11, polyorder=3)\n",
    "\n",
    "    # get max gradient so we're in the pedestal\n",
    "    p_s_grad = np.gradient(p_s)\n",
    "    min_i = np.argmin(p_s_grad)\n",
    "\n",
    "\n",
    "    # search from pedestal region outward in 2nd derivatives\n",
    "    p_s_grad2 = np.gradient(p_s_grad)\n",
    "    p_s_grad2 = savgol_filter(p_s_grad2, window_length=11, polyorder=3)  # aggressively smooth as well\n",
    "    p_s_grad2 = standardize_signal(p_s_grad2, trim_zeros=True)\n",
    "    # standardize s.t. if we go >1 sd up/down, we stop searching\n",
    "\n",
    "    # go to the left from middle point\n",
    "    sd_cutoff = -.5\n",
    "    start_cut_early = False  # if we go < -1 for x'', mark as such, such that if we go > -1 again we stop looking\n",
    "    min_val = p_s_grad2[min_i]\n",
    "    top_i = min_i\n",
    "    for i in reversed(range(0, min_i)):\n",
    "        if p_s_grad2[i] < min_val:\n",
    "            min_val = p_s_grad2[i]\n",
    "            top_i = i\n",
    "            if min_val < sd_cutoff:\n",
    "                start_cut_early = True\n",
    "        elif start_cut_early and p_s_grad2[i] > sd_cutoff:\n",
    "            break\n",
    "\n",
    "    # found our x for top\n",
    "    # same procedure for bottom\n",
    "    sd_cutoff = .5\n",
    "    start_cut_early = False  # if we go > 1 for x'', etc.\n",
    "    max_val = p_s_grad2[min_i]\n",
    "    bottom_i = min_i\n",
    "\n",
    "    for i in range(min_i+1, x_h.shape[0]):\n",
    "        if p_s_grad2[i] > max_val:\n",
    "            max_val = p_s_grad2[i]\n",
    "            bottom_i = i\n",
    "            if max_val > sd_cutoff:\n",
    "                start_cut_early = True\n",
    "        elif start_cut_early and p_s_grad2[i] < sd_cutoff:\n",
    "            break\n",
    "\n",
    "    top, bottom = x_h[top_i], x_h[bottom_i]\n",
    "\n",
    "    if plot_result:\n",
    "        plt.plot(x_h, p_s)\n",
    "        plt.axvline(top, color='red', label='estimated top')\n",
    "        plt.axvline(bottom, color='blue', label='estimated bottom')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return top, bottom\n",
    "\n",
    "def find_separatrix(ne, te, x, plot_result=False):\n",
    "    top_x, bottom_x = pedestal_top(ne, x, plot_result=False)\n",
    "    \n",
    "    # initial estimate\n",
    "    x_sep = (1/4) * top_x + (3/4) * bottom_x\n",
    "    x_initial = x_sep\n",
    "    # scan area around x_sep\n",
    "    # first, interp to higher space\n",
    "    x_h = np.linspace(x[0], x[-1], 200)  # interpolate to 50 (evenly spaced) points\n",
    "    \n",
    "    f_ne_interp = interpolate.interp1d(x, ne)\n",
    "    ne_h = f_ne_interp(x_h)\n",
    "    ne_s = savgol_filter(ne_h, window_length=11, polyorder=3)\n",
    "    f_te_interp = interpolate.interp1d(x, te)\n",
    "    te_h = f_te_interp(x_h)\n",
    "    te_s = savgol_filter(te_h, window_length=11, polyorder=3)\n",
    "    \n",
    "    # closest x\n",
    "    s_i = find_nearest(x_sep, x_h)\n",
    "    # scan inwards/outwards\n",
    "    n_step = 10\n",
    "    te_target = 100 # 100 eV\n",
    "    te_distr = norm(te_target, 10)  # distribution to weigh new points against\n",
    "    weight_func = lambda x: x ** 0.05\n",
    "    \n",
    "    def te_sep():\n",
    "        idx = get_nearest_weighted_idx(x_sep, x_h, sort=False)\n",
    "        val_out = 0\n",
    "        for w, w_i in idx:\n",
    "            val_out += w * te_s[w_i]\n",
    "        return val_out\n",
    "    \n",
    "    for i in range(1, n_step+1):\n",
    "        i_left = s_i - i\n",
    "        i_right = s_i + i\n",
    "        if i_left < 0 or i_right > x_h.shape[0] - 1:\n",
    "            break  # reached outside the grid on 1 side\n",
    "        weight_pos = i / n_step  # [1/n, 1] adjustment based on how far the value is 'good'\n",
    "        pdf_l = te_distr.pdf(te_s[i_left])\n",
    "        adjust_l = weight_pos * pdf_l\n",
    "        adjust_l = weight_func(adjust_l)\n",
    "        if te_s[i_left] > te_target and te_sep() < te_target:  # on the left is above 100 --> we need to move left\n",
    "            x_sep = (x_sep + x_h[i_left] * adjust_l) / (1 + adjust_l)\n",
    "\n",
    "        pdf_r = te_distr.pdf(te_s[i_right])\n",
    "        adjust_r = weight_pos * pdf_r\n",
    "        adjust_r = weight_func(adjust_r)\n",
    "        if te_s[i_right] < te_target and te_sep() > te_target:  # on the right is below 100 --> we need to move right\n",
    "            x_sep = (x_sep + x_h[i_right] * adjust_r) / (1 + adjust_r)\n",
    "    \n",
    "    # closest_x_index = np.argmin(abs(x_h - x_sep))\n",
    "    # from_fit = x_h[closest_x_index], te_h[closest_x_index], ne_h[closest_x_index]\n",
    "    interp_idx_l, interp_idx_r = get_idx_for_linear_interp(x_sep, x, te)\n",
    "    weights_r, weights_l = get_weights(te, interp_idx_l, interp_idx_r)\n",
    "    tesep_estimation = weights_l*te[interp_idx_l] + weights_r*te[interp_idx_r]\n",
    "    nesep_estimation = weights_l*ne[interp_idx_l] + weights_r*ne[interp_idx_r]\n",
    "    rsep_estimation = weights_l*x[interp_idx_l] + weights_r*x[interp_idx_r]\n",
    "    if nesep_estimation < 0: \n",
    "        plot_result = True\n",
    "    if plot_result:\n",
    "        fig, axs=plt.subplots(1, 2, figsize=(6,3))\n",
    "        axs[0].plot(x_h, te_s)\n",
    "        axs[0].scatter(x, te)\n",
    "        axs[0].scatter(x[interp_idx_l], te[interp_idx_l], color='orange')\n",
    "        axs[0].scatter(x[interp_idx_r], te[interp_idx_r], color='orange')\n",
    "        \n",
    "        axs[0].axvline(x_initial, color='red', label='initial estimate')\n",
    "        axs[0].axvline(x_sep, color='green', label='final estimate')\n",
    "        axs[0].axhline(100, color='black', label='100eV')\n",
    "        axs[0].scatter(rsep_estimation, tesep_estimation, color='black',marker='*', s=250)\n",
    "\n",
    "        axs[1].plot(x_h, ne_s)\n",
    "        axs[1].scatter(x, ne)\n",
    "        axs[1].scatter(rsep_estimation, nesep_estimation, color='black',marker='*', s=250)\n",
    "        axs[1].axvline(x_initial, color='red', label='initial prior')\n",
    "        axs[1].axvline(x_sep, color='green', label='final prior')\n",
    "        axs[1].axvline(rsep_estimation, color='black', ls='--', label='final est')\n",
    "        axs[1].scatter(x[interp_idx_l], ne[interp_idx_l], color='orange')\n",
    "        axs[1].scatter(x[interp_idx_r], ne[interp_idx_r], color='orange')\n",
    "        axs[1].legend()\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return tesep_estimation, nesep_estimation, rsep_estimation\n",
    "\n",
    "def get_idx_for_linear_interp(x_sep, x, te): \n",
    "    closest_point = np.argmin(abs(x - x_sep))\n",
    "    if (te < 100).sum() == 0: \n",
    "        closest_point = np.argmin(abs(te - 100))\n",
    "        idx_r = closest_point\n",
    "        idx_l = idx_r - 1\n",
    "   \n",
    "    elif te[closest_point] < 100: \n",
    "        idx_r = closest_point\n",
    "        idx_l = closest_point - 1\n",
    "        while te[idx_l] < 100: \n",
    "            idx_r = idx_l\n",
    "            idx_l = idx_l - 1\n",
    "    else: \n",
    "        idx_r = closest_point + 1\n",
    "        idx_l = closest_point\n",
    "        while te[idx_r] > 100:\n",
    "            idx_l = idx_r\n",
    "            idx_r = idx_l + 1\n",
    "            \n",
    "    return idx_l, idx_r \n",
    "def find_elm_percent(pulse_num, time): \n",
    "    try: \n",
    "        pulse_elm_timings_frass = np.array(JET_ELM_TIMINGS[pulse_num])\n",
    "    except KeyError as e:\n",
    "        return np.nan\n",
    "    diff = pulse_elm_timings_frass - time\n",
    "    try:\n",
    "        time_last_elm = pulse_elm_timings_frass[diff < 0][-1]\n",
    "        time_next_elm = pulse_elm_timings_frass[diff > 0][0]\n",
    "        elm_percent = (time - time_last_elm) / (time_next_elm - time_last_elm)\n",
    "    except IndexError as e:\n",
    "        elm_percent = np.nan\n",
    "    return elm_percent\n",
    "def get_weights(te, idx_l, idx_r, query=100): \n",
    "    dist = te[idx_r] - query + query - te[idx_l]\n",
    "    weights = (1-(te[idx_r] - query)/dist, 1-(query - te[idx_l])/dist) \n",
    "    return weights\n",
    "def get_lorenzo_pred(pulse_num, pulse_time): \n",
    "    JPDB_pulse = PULSE_DF_SANDBOX[PULSE_DF_SANDBOX['shot'] == pulse_num]\n",
    "    if len(JPDB_pulse) > 1: \n",
    "        print(JPDB_pulse[['t1', 't2']])\n",
    "        for local_pulse in JPDB_pulse: \n",
    "            t1, t2 = local_pulse[['t1', 't2']]\n",
    "            if t1 < pulse_time and t2 > pulse_time: \n",
    "                nesep_exp, nesep_lor = local_pulse[['neseparatrixfromexpdata10^19(m^-3)', 'neseparatrixfromfit10^19(m^-3)']].values[0]\n",
    "            else: \n",
    "                continue\n",
    "    else: \n",
    "        index_to_take_from = 0\n",
    "        nesep_exp, nesep_lor = JPDB_pulse[['neseparatrixfromexpdata10^19(m^-3)', 'neseparatrixfromfit10^19(m^-3)']].values[0]\n",
    "        \n",
    "    return nesep_exp, nesep_lor\n",
    "      \n",
    "# grab example data\n",
    "\n",
    "# profiles, mps, masks, psis, rmids, trainids, uncerts\n",
    "\n",
    "\n",
    "our_vals, lor_vals = [], []\n",
    "for idx in range(0, 1):\n",
    "    name = ids[idx]\n",
    "    pulse_num, time = name.split('/')\n",
    "    _, lor_val = get_lorenzo_pred(int(pulse_num), time)\n",
    "    original = profiles[idx][0], profiles[idx][1],uncerts[idx][0], uncerts[idx][1], rmids[idx]\n",
    "    ne, te, dne, dte, x = original\n",
    "    new_ne, new_te =  np.minimum.accumulate(ne), np.minimum.accumulate(te)\n",
    "    keep_idx = np.where(new_te == te)\n",
    "    \n",
    "    ne, te, dne, dte, x = new_ne[keep_idx], new_te[keep_idx], dne[keep_idx], dte[keep_idx], x[keep_idx]\n",
    "    # logical_bool_mask = te < 700\n",
    "    logical_bool_mask = np.logical_and(ne > 0, te > 0)\n",
    "    logical_bool_mask = np.logical_and(logical_bool_mask, dte > 0)\n",
    "    logical_bool_mask = np.logical_and(logical_bool_mask, dne > 0)\n",
    "    logical_bool_mask = np.logical_and(logical_bool_mask, dte < 3000)\n",
    "    \n",
    "    # check if te below 100! \n",
    "    if (te < 100).sum() == 0: \n",
    "        continue\n",
    "        \n",
    "    top_x, bottom_x = pedestal_top(ne, x, plot_result=False)\n",
    "    try: \n",
    "        estimations = find_separatrix(ne, te, x, plot_result=False)\n",
    "    except IndexError as e:\n",
    "        print('IDX ERROR', name, idx)\n",
    "        fig, axs=plt.subplots(1, 2, figsize=(6,3))\n",
    "        axs[0].scatter(x, te)\n",
    "        \n",
    "        axs[0].axhline(100, color='black', label='100eV')\n",
    "        axs[1].scatter(x, ne)\n",
    "        axs[1].legend()\n",
    "        \n",
    "        plt.show()\n",
    "    else:\n",
    "        if estimations[1] < 0.6e19: \n",
    "            print(estimations[1])\n",
    "            find_separatrix(ne, te, x, plot_result=True)\n",
    "        our_vals.append(estimations[1])\n",
    "        lor_vals.append(lor_val)\n",
    "    # print(abs(estimations[1]*1e-19 - lor_val))\n",
    "lor_vals, our_vals = np.array(lor_vals), 1e-19*np.array(our_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "296ebc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_used_dict(pulse_num, dict_to_update, initialize=False, **kwargs): \n",
    "    if pulse_num not in dict_to_update.keys(): \n",
    "        initialize = True\n",
    "        dict_to_update[pulse_num] = {}\n",
    "    for key, item in kwargs.items(): \n",
    "        if initialize: \n",
    "            dict_to_update[pulse_num][key] = []\n",
    "        dict_to_update[pulse_num][key].append(item)\n",
    "    return dict_to_update\n",
    "\n",
    "def clean_used_dict(dict_to_update):\n",
    "    updated_dict = dict_to_update.copy()\n",
    "    for pulse_num, pulse_items in updated_dict.items(): \n",
    "        # Find a bool array list of indexes that should be kept. \n",
    "        nesep_array = np.array(pulse_items['neseps'])\n",
    "        nesep_mean, nesep_std = nesep_array.mean(), nesep_array.std()\n",
    "        slices_to_keep = abs(nesep_array - nesep_mean) < nesep_std\n",
    "        for key, item in pulse_items.items():\n",
    "            updated_dict[pulse_num][key] = [it for n, it in enumerate(item) if slices_to_keep[n]] # np.array(item)[slices_to_keep]\n",
    "    return updated_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fcbbfb2f-8840-4f6a-9682-f64e5ea148b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0889ad8a35d749e1b1ad51b354190bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31335 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "\n",
    "def make_dataset(all_dataset):\n",
    "    # Store MPS in big table\n",
    "    # Store Targets (nesep) in array \n",
    "    # Store #IDS for lookup later\n",
    "    profiles, rmids, ids, uncerts, mps = all_dataset \n",
    "    iterator = tqdm(range(len(profiles)))\n",
    "    used_dict = {}\n",
    "    \n",
    "    for idx in iterator: \n",
    "        name = ids[idx]\n",
    "        pulse_num, time = name.split('/')\n",
    "        elm_perc = find_elm_percent(int(pulse_num), float(time))\n",
    "        if np.isnan(elm_perc): \n",
    "            continue\n",
    "        else: \n",
    "            # used_elm_percents.append(elm_perc)\n",
    "            pass\n",
    "        _, lor_val = get_lorenzo_pred(int(pulse_num), time)\n",
    "            \n",
    "        original = profiles[idx][0], profiles[idx][1],uncerts[idx][0], uncerts[idx][1], rmids[idx]\n",
    "        find_elm_percent(pulse_num, time)\n",
    "        ne, te, dne, dte, x = original\n",
    "        new_ne, new_te =  np.minimum.accumulate(ne), np.minimum.accumulate(te)\n",
    "        keep_idx = np.where(new_te == te)\n",
    "\n",
    "        ne, te, dne, dte, x = new_ne[keep_idx], new_te[keep_idx], dne[keep_idx], dte[keep_idx], x[keep_idx]\n",
    "        logical_bool_mask = np.logical_and(ne > 0, te > 0) # check that nothing is below zero, i mean what the fuck\n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dte > 0) # Also here\n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dne > 0) # Also here \n",
    "        logical_bool_mask = np.logical_and(logical_bool_mask, dte < 3000) # Don't want tesep values that are ridonklus\n",
    "        ne, te, dne, dte, x = ne[logical_bool_mask], te[logical_bool_mask], dne[logical_bool_mask], dte[logical_bool_mask], x[logical_bool_mask]\n",
    "        if (te < 100).sum() == 0:\n",
    "            continue\n",
    "        else: \n",
    "            try: \n",
    "                estimations = find_separatrix(ne, te, x, plot_result=False)\n",
    "            except IndexError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            else: \n",
    "                used_dict = extend_used_dict(pulse_num, used_dict, profiles=profiles[idx], neseps=estimations[1], ids=name, mps=mps[idx], elm_perc=elm_perc, lor_val=lor_val, rmids=rmids[idx])\n",
    "    cleaned_dict = clean_used_dict(used_dict)\n",
    "    return cleaned_dict\n",
    "\n",
    "supervised_dict = make_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c9103001-27e6-45f7-9be2-f8536650f31f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16111\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# with open('../../../moxie/data/processed/supervised_dict_collab_160522.pickle', 'wb') as file: \n",
    "#     pickle.dump(supervised_dict, file)\n",
    "total_slices = 0\n",
    "max_prof_length = 0\n",
    "for pulse_num in supervised_dict.keys(): \n",
    "    total_slices += len(supervised_dict[pulse_num]['ids'])\n",
    "    for prof in supervised_dict[pulse_num]['profiles']: \n",
    "        max_prof_length = max(max_prof_length, len(prof[0]))\n",
    "print(total_slices)\n",
    "print(max_prof_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8b2e4ac1-9495-402c-98b1-2d885331e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "def get_pulse_train_val_split(sandbox_dict): \n",
    "    train_size = 0.85\n",
    "    val_size = 0.2\n",
    "    pulse_nums = list(sandbox_dict.keys())\n",
    "    \n",
    "    k_train = int(len(pulse_nums) * train_size)\n",
    "    \n",
    "    indicies = random.sample(range(len(pulse_nums)), k_train)\n",
    "    \n",
    "    train_val_pulses = [pulse_nums[i] for i in indicies]\n",
    "    test_pulses = list(set(pulse_nums) - set(train_val_pulses))\n",
    "    \n",
    "    k_val = int(len(train_val_pulses) * val_size)\n",
    "    indicies = random.sample(range(len(train_val_pulses)), k_val)\n",
    "    \n",
    "    val_pulses = [train_val_pulses[i] for i in indicies]\n",
    "    \n",
    "    \n",
    "    train_pulses = list(set(train_val_pulses) - set(val_pulses))\n",
    "        \n",
    "    assert len(pulse_nums) == (len(train_pulses) + len(val_pulses) + len(test_pulses))\n",
    "    return train_pulses, val_pulses, test_pulses\n",
    "\n",
    "def pad_lists(list_to_pad, key, pad_length=19, **kwargs): \n",
    "    if key == 'profiles': \n",
    "        padded_profiles = np.zeros((len(list_to_pad), 2, pad_length))\n",
    "        for n, prof in enumerate(list_to_pad): \n",
    "            num_missing = pad_length - len(prof[0])\n",
    "            padded_prof = np.pad(prof, ((0,0), (num_missing,0)), mode='edge')\n",
    "            padded_profiles[n] = padded_prof\n",
    "        return padded_profiles\n",
    "    elif key == 'rmids': \n",
    "        padded_radii =  np.zeros((len(list_to_pad), 19))\n",
    "        for n, rmid in enumerate(list_to_pad): \n",
    "            num_missing = pad_length - len(rmid)\n",
    "            padded_rmid = np.pad(rmid, (num_missing,0), mode='constant') \n",
    "            padded_radii[n] = padded_rmid\n",
    "        return padded_radii\n",
    "    elif key == 'masks': \n",
    "        padded_masks = np.zeros((len(list_to_pad), 19)) \n",
    "        for n, mask in enumerate(list_to_pad): \n",
    "            num_missing = pad_length - len(mask)\n",
    "            padded_mask = np.pad(mask, (num_missing, 0), mode='constant', constant_values=False)\n",
    "            padded_masks[n] = padded_mask\n",
    "        return padded_masks\n",
    "def make_subset(sandbox_dict, pulse_list): \n",
    "    subset_dict = {}\n",
    "    for pulse_num in pulse_list: \n",
    "        pulse_dict = sandbox_dict[pulse_num]\n",
    "        for key, item in pulse_dict.items(): \n",
    "            if key not in subset_dict.keys(): \n",
    "                subset_dict[key] = []\n",
    "            if key in ['profiles', 'rmids', 'masks']: \n",
    "                padded_item = pad_lists(item, key)\n",
    "            else: \n",
    "                padded_item = np.array(item)\n",
    "            subset_dict[key].append(padded_item)\n",
    "    for key in subset_dict.keys(): \n",
    "        if key in ['profiles', 'rmids', 'masks', 'mps']: \n",
    "            subset_dict[key] = np.vstack(subset_dict[key])\n",
    "        else: \n",
    "            subset_dict[key] = np.concatenate(subset_dict[key])\n",
    "    return subset_dict\n",
    "\n",
    "def make_train_val_test_sets(sandbox_dict, train_pulses, val_pulses, test_pulses): \n",
    "    train_dict = make_subset(sandbox_dict, train_pulses)\n",
    "    val_dict = make_subset(sandbox_dict, val_pulses)\n",
    "    test_dict = make_subset(sandbox_dict, test_pulses)\n",
    "    return {'train': train_dict, 'val': val_dict, 'test': test_dict}\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "91b3e372-0ea0-4736-a1dc-b4cf81984639",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pulses, val_pulses, test_pulses = get_pulse_train_val_split(supervised_dict)\n",
    "full_dict = make_train_val_test_sets(supervised_dict, train_pulses, val_pulses, test_pulses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "151069fa-f1bb-4b68-b95e-471b787311eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10892, 2, 19) (10892, 13) (10892,)\n"
     ]
    }
   ],
   "source": [
    "print(full_dict['train']['profiles'].shape, full_dict['train']['mps'].shape, full_dict['train']['elm_perc'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3867117c-d6f6-4fd4-89ca-3f316cc36e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncax = plt.scatter(lor[mask], 1e-19*ours[mask], c=elm[mask])\\nplt.plot(lor[mask],lor[mask], color='black', ls='--')\\nplt.xlabel('Frassinetti Value')\\nplt.ylabel('Slice Value')\\nplt.colorbar(cax, label='ELM PERC')\\nplt.show()\\n\\n\\ncax = plt.scatter(reduced_lor, 1e-19*np.array(reduced_ours))\\nplt.plot(reduced_lor,reduced_lor, color='black', ls='--')\\nplt.xlabel('Frassinetti Value')\\nplt.ylabel('Slice Value')\\n# plt.colorbar(cax, label='ELM PERC')\\nplt.show()\\n\\nfig = plt.figure()\\nplt.hist(reduced_ours, density=True, bins=20)\\nplt.show()\\n\\nfig = plt.figure()\\nplt.hist(ours[mask], density=True, bins=20)\\nplt.show()\\n\\nfig = plt.figure()\\nplt.hist(ours_std_by_pulse, density=True, bins=20)\\nplt.xlabel('Standard Deviations')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lor, ours, elm, ids = supervised_dict['lor'], supervised_dict['neseps'], supervised_dict['elms'], np.array(supervised_dict['ids'])\n",
    "\n",
    "\n",
    "\n",
    "# mask = elm > 0.75\n",
    "# mask = np.logical_and(mask, ours > 0.5e19)\n",
    "# mask = np.logical_and(mask, lor > 2.5)\n",
    "# mask = np.logical_and(mask, lor < 3.5)\n",
    "\n",
    "masked_ids = ids# [mask]\n",
    "masked_lor = lor# [mask]\n",
    "masked_ours = ours# [mask]\n",
    "order = [int(x.split('/')[0]) for x in masked_ids]\n",
    "pulses = unique(order)\n",
    "idxs = [[index for index in range(len(order)) if order[index] == pulse] for pulse in pulses]\n",
    "\n",
    "ours_by_pulse = [np.array([masked_ours[idx] for idx in pulse_idxs]) for pulse_idxs in idxs]\n",
    "ours_std_by_pulse = [np.array([masked_ours[idx] for idx in pulse_idxs]).std() for pulse_idxs in idxs]\n",
    "ours_mean_by_pulse = [np.array([masked_ours[idx] for idx in pulse_idxs]).mean() for pulse_idxs in idxs]\n",
    "\n",
    "lor_by_pulse = [np.array([masked_lor[idx] for idx in pulse_idxs]) for pulse_idxs in idxs]\n",
    "mp_by_pulse = [np.array([dataset[-1][idx] for idx in pulse_idxs]) for pulse_idxs in idxs]\n",
    "elm_by_pulse = [np.array([elm[idx] for idx in pulse_idxs]) for pulse_idxs in idxs]\n",
    "reduced_ours = []\n",
    "reduced_lor = []\n",
    "reduced_mps = []\n",
    "reduced_elms = []\n",
    "for n, (sub_pulse, lor_pulse, mps_pulse, elm_pulse) in enumerate(zip(ours_by_pulse, lor_by_pulse, mp_by_pulse, elm_by_pulse)):\n",
    "    pulse_std, pulse_mean = ours_std_by_pulse[n], ours_mean_by_pulse[n]\n",
    "    if pulse_std > 0.5e19: pulse_std = 0.5e19\n",
    "    for val, val_lor, mp, el in zip(sub_pulse, lor_pulse,mps_pulse, elm_pulse): \n",
    "        if val > pulse_mean + pulse_std or val < pulse_mean  - pulse_std: \n",
    "            continue \n",
    "        else: \n",
    "            reduced_mps.append(mp)\n",
    "            reduced_ours.append(val)\n",
    "            reduced_lor.append(val_lor)\n",
    "            reduced_elms.append(el)\n",
    "\"\"\"\n",
    "cax = plt.scatter(lor[mask], 1e-19*ours[mask], c=elm[mask])\n",
    "plt.plot(lor[mask],lor[mask], color='black', ls='--')\n",
    "plt.xlabel('Frassinetti Value')\n",
    "plt.ylabel('Slice Value')\n",
    "plt.colorbar(cax, label='ELM PERC')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "cax = plt.scatter(reduced_lor, 1e-19*np.array(reduced_ours))\n",
    "plt.plot(reduced_lor,reduced_lor, color='black', ls='--')\n",
    "plt.xlabel('Frassinetti Value')\n",
    "plt.ylabel('Slice Value')\n",
    "# plt.colorbar(cax, label='ELM PERC')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(reduced_ours, density=True, bins=20)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(ours[mask], density=True, bins=20)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(ours_std_by_pulse, density=True, bins=20)\n",
    "plt.xlabel('Standard Deviations')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38a5f1ca-6d89-4c7e-8f45-53a7c573f577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(sequence):\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe709fd6-c03a-473e-9b8b-fd1ab6534fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_supervised_dict = {'controls': np.array(reduced_mps), 'neseps': np.array(reduced_ours), 'elms': np.array(reduced_elms), 'lor': np.array(reduced_lor)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a77f18e-4450-4521-b372-0fd54df0cc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../../moxie/data/processed/supervised_dict_reduced_170522.pickle', 'wb') as file: \n",
    "    pickle.dump(reduced_supervised_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0c1f6-7a05-43c6-adab-0b9b659eec40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
