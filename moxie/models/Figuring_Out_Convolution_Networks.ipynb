{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f338e594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3daad609",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/home/kitadam/ENR_Sven/moxie/data/processed/pedestal_profile_for_nesep.hdf5', 'r') as file:\n",
    "    X, y = file['strohman']['X'][:], file['strohman']['y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86c408d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = torch.from_numpy(X), torch.from_numpy(y)\n",
    "# A pseudo batch \n",
    "X, y = X[:512], y[:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5000bfde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 63])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b8c0c",
   "metadata": {},
   "source": [
    "The dimensions need to be expanded, as we Conv layers expect a shape of `[batch_size, channels, length]`\n",
    "\n",
    "For us, the channels is 1, for 1 type of time series (at the moment just density profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7ab5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b04bd1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 63])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6c6056",
   "metadata": {},
   "source": [
    "Now lets make some convolution layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "970f39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1 = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, stride=1)\n",
    "conv_2 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, stride=1)\n",
    "conv_3 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b493d1a",
   "metadata": {},
   "source": [
    "We pass the through each conv layer, printing the size as we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "005f005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First conv torch.Size([512, 4, 61])\n",
      "Second conv torch.Size([512, 8, 59])\n",
      "Third conv torch.Size([512, 16, 57])\n"
     ]
    }
   ],
   "source": [
    "out_1 = conv_1(X)\n",
    "print('First conv {}'.format(out_1.shape))\n",
    "out_2 = conv_2(out_1)\n",
    "print('Second conv {}'.format(out_2.shape))\n",
    "out_3 = conv_3(out_2)\n",
    "print('Third conv {}'.format(out_3.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad36411c",
   "metadata": {},
   "source": [
    "As you can see, the length is slowly decreasing! We can change this to be constant to the relative shape of it all. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4b8f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1 = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=3, stride=1, padding='same')\n",
    "conv_2 = nn.Conv1d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding='same')\n",
    "conv_3 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "438f0823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First conv torch.Size([512, 4, 63])\n",
      "Second conv torch.Size([512, 8, 63])\n",
      "Third conv torch.Size([512, 16, 63])\n"
     ]
    }
   ],
   "source": [
    "out_1 = conv_1(X)\n",
    "print('First conv {}'.format(out_1.shape))\n",
    "out_2 = conv_2(out_1)\n",
    "print('Second conv {}'.format(out_2.shape))\n",
    "out_3 = conv_3(out_2)\n",
    "print('Third conv {}'.format(out_3.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee82e4",
   "metadata": {},
   "source": [
    "Now we want to be able to feed this to our latent space, which means we have to flatten it back to its original shape `[batch_size, new_length]`, where new length will be our last channel size times our input vector size, which is in our case `63*16=1008`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd17a52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1008])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = out_3.view(out_3.size(0),-1)\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de63438",
   "metadata": {},
   "source": [
    "Now we represent the latent space, which has hidden dimension 10,  with the variables mu and logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "082f8630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 10]), torch.Size([512, 10]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_mu, fc_logvar = nn.Linear(1008, 10), nn.Linear(1008, 10)\n",
    "mu, logvar = fc_mu(encoded), fc_logvar(encoded)\n",
    "mu.shape, logvar.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363a388e",
   "metadata": {},
   "source": [
    "Now we reparameterize the mu and logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "285dd1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std = torch.exp(0.5*logvar)\n",
    "eps = torch.randn_like(std)\n",
    "z = eps * std + mu\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32e45c",
   "metadata": {},
   "source": [
    "Now we want to feed it back into the transposed convolution layers, so we need to go from our hidden dimension 10 back to 1008 `(16*63)` using a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9cc6e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1008])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = nn.Linear(10, 1008)\n",
    "decoder_ready = decoder_input(z)\n",
    "decoder_ready.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32361f34",
   "metadata": {},
   "source": [
    "But we have to do the inverse of before so we have to unflatten it to feed back into the convolution steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f9aa5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 16, 63])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unflattened = decoder_ready.view(decoder_ready.size(0), 16, 63)\n",
    "unflattened.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736ac35",
   "metadata": {},
   "source": [
    "Now we design our 3 deconvolution steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b581ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_conv3 = nn.ConvTranspose1d(in_channels=16, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "t_conv2 = nn.ConvTranspose1d(in_channels=8, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
    "t_conv1 = nn.ConvTranspose1d(in_channels=4, out_channels=1, kernel_size=3, stride=1, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72af1f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First transposed conv torch.Size([512, 8, 63])\n",
      "Second transposed conv torch.Size([512, 4, 63])\n",
      "First transposed conv torch.Size([512, 1, 63])\n"
     ]
    }
   ],
   "source": [
    "d_out3 = t_conv3(unflattened)\n",
    "print('First transposed conv {}'.format(d_out3.shape))\n",
    "d_out2 = t_conv2(d_out3)\n",
    "print('Second transposed conv {}'.format(d_out2.shape))\n",
    "d_out1 = t_conv1(d_out2)\n",
    "print('First transposed conv {}'.format(d_out1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6895d92",
   "metadata": {},
   "source": [
    "We could run this through another few linear layers of arbitrary size, but the output is how we want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02a1a759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1, 63])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_layer = nn.Sequential(nn.Linear(63, 63), nn.Linear(63, 63))\n",
    "final = final_layer(d_out1)\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd5016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
