{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462785c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f94eb8",
   "metadata": {},
   "source": [
    "- Gather pulses, t1, t2, nesep (exp & from fit) from JET database\n",
    "    - If nesep is 0, then drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_pdb = pd.read_csv('pedestal-database.csv')\n",
    "important_info = jet_pdb[['shot', 't1', 't2', 'neseparatrixfromexpdata10^19(m^-3)', 'error_neseparatrixfromexpdata10^19(m^-3)','neseparatrixfromfit10^19(m^-3)',\n",
    " 'error_neseparatrixfromfit10^19(m^-3)','FLAG:HRTSdatavalidated']]\n",
    "final_pulse_list = important_info[(important_info['neseparatrixfromexpdata10^19(m^-3)'] != 0.0) & (important_info['neseparatrixfromexpdata10^19(m^-3)'] != -1.0) &  (important_info['shot'] >= 79000) &  (important_info['FLAG:HRTSdatavalidated'] > 0)]\n",
    "\n",
    "# final_pulse_list.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95394e0",
   "metadata": {},
   "source": [
    "- Import the raw profiles stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06dd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_shots_with_T.pickle', 'rb') as file:\n",
    "    pulse_dicts = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea577183",
   "metadata": {},
   "source": [
    "Create HDF5 file with the dataset groups (Name of group): \n",
    "\n",
    "1. (only_density) Density profiles in the time window \n",
    "2. (density_and_temperature) Density and Temperature profiles\n",
    "\n",
    "For each group we have the following datasets \n",
    "- X: profiles\n",
    "- y: corresponding machine parameters for time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ffa5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('../processed/pedestal_profile_dataset_v3.hdf5', 'w')\n",
    "grp_strohman = f.create_group(\"strohman\")\n",
    "grp_both = f.create_group(\"density_and_temperature\")\n",
    "\n",
    "grp_str_train = grp_strohman.create_group('train')\n",
    "grp_str_test = grp_strohman.create_group('test')\n",
    "grp_str_val = grp_strohman.create_group('valid')\n",
    "\n",
    "grp_both_train = grp_both.create_group('train')\n",
    "grp_both_test = grp_both.create_group('test')\n",
    "grp_both_val = grp_both.create_group('valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fa9e3",
   "metadata": {},
   "source": [
    "### Strohman Gathering\n",
    "\n",
    "- For each shot in pedestal db\n",
    "    - Gather density profiles within time windows of entry\n",
    "    - Add them to 70% to train, 10% to valid, 20% to test\n",
    "    - For each input variable in list, get average of the value for given time window\n",
    "    - Get nesep and put it also into y\n",
    "    - put into y\n",
    "    \n",
    "### Extended Gathering\n",
    "- Same as above, but take density as well, and have it be a 2D input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "EFIT = ['Q95', 'RGEO', 'CRO', 'VOLM', 'TRIU', 'TRIL', 'XIP', 'ELON', 'POHM']\n",
    "ALL_KEYS = EFIT + ['BT', 'ELER', 'P_NBI', 'P_ICRH', 'NE']\n",
    "def sample_nesep(mean_val, std_val):\n",
    "    return np.random.normal(mean_val, std_val)\n",
    "\n",
    "def sample_input(input_dict, key, t1, t2, nesep):\n",
    "    if key in EFIT: \n",
    "        if len(input_dict[key]) == 0:\n",
    "            return -1 \n",
    "        time_idx = np.logical_and(input_dict['EFIT_T'] > t1, input_dict['EFIT_T'] < t2)\n",
    "        sample = np.mean(np.array(input_dict[key])[time_idx])\n",
    "    else:\n",
    "        if key == 'NE':\n",
    "            return nesep\n",
    "        else:\n",
    "            time_idx = np.logical_and(input_dict[key]['time'] > t1, input_dict[key]['time'] < t2)\n",
    "            sample = np.mean(np.array(input_dict[key]['values'])[time_idx])\n",
    "    return sample\n",
    "\n",
    "def get_ne_and_te_profiles(raw_shot, t1, t2):\n",
    "    sample_ne = raw_shot['outputs']['NE']\n",
    "    sample_te = raw_shot['outputs']['TE']\n",
    "    sample_time = raw_shot['outputs']['time']\n",
    "    profiles_idx = np.logical_and(sample_time > t1, sample_time < t2)\n",
    "    profiles_ne = sample_ne[profiles_idx]\n",
    "    profiles_te = sample_te[profiles_idx]\n",
    "    \n",
    "    if len(profiles_ne[0]) != 63:\n",
    "        profiles_ne = np.pad(profiles_ne, ((0, 0), (0, 63 - len(profiles_ne[0]))), 'constant')\n",
    "    if len(profiles_te[0]) != 63:\n",
    "        profiles_te = np.pad(profiles_te, ((0, 0), (0, 63 - len(profiles_te[0]))), 'constant')\n",
    "    \n",
    "    # Returns to numpy arrays of the same shape\n",
    "    return profiles_ne, profiles_te\n",
    "\n",
    "def combine_ne_te(prof_ne, prof_te):\n",
    "    \n",
    "    combined = np.stack([prof_ne, prof_te], axis=1)\n",
    "    # Want a shape of num_slices X 63\n",
    "    return combined\n",
    "\n",
    "def sample_slices(slices, y):\n",
    "    num_windows = len(slices)\n",
    "        \n",
    "    test_size = int(0.2*num_windows)\n",
    "    val_size = int(0.1*num_windows)\n",
    "    \n",
    "    if val_size == 0 or test_size==0:\n",
    "        val_size = 1\n",
    "        test_size = 1\n",
    "        \n",
    "    train_size = num_windows - test_size - val_size\n",
    "    \n",
    "    X_train, X_test, y_train, y_test =  train_test_split(slices, y, test_size=test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size)\n",
    "    \n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "pulses = pulse_dicts.keys()\n",
    "\n",
    "X_train_str, y_train_str = [], []\n",
    "X_val_str, y_val_str = [], []\n",
    "X_test_str, y_test_str = [], []\n",
    "\n",
    "X_train_both, y_train_both = [], []\n",
    "X_val_both, y_val_both = [], []\n",
    "X_test_both, y_test_both = [], []\n",
    "\n",
    "\n",
    "pulse_list = []\n",
    "i = 0\n",
    "for index, row in final_pulse_list.iterrows():\n",
    "    shot, t1, t2 = str(int(row['shot'])), row['t1'], row['t2']\n",
    "    nesep, dnesep = row['neseparatrixfromexpdata10^19(m^-3)'], row['error_neseparatrixfromexpdata10^19(m^-3)']\n",
    "    if shot not in pulses:\n",
    "        print('Shot {} not stored in adams db, you should probably go get it'.format(shot))\n",
    "        continue\n",
    "    raw_shot = pulse_dicts[shot]\n",
    "    pulse_list.append(shot)\n",
    "    \n",
    "    profiles_ne, profiles_te = get_ne_and_te_profiles(raw_shot, t1, t2)\n",
    "    combined = combine_ne_te(profiles_ne, profiles_te)\n",
    "    other_params = np.array([[sample_input(raw_shot['inputs'], key, t1, t2, nesep) for key in ALL_KEYS] for _ in range(len(profiles_ne))])\n",
    "    \n",
    "    ne_train, ne_test, ne_valid, ne_y_train, ne_y_test, ne_y_val = sample_slices(profiles_ne, other_params)\n",
    "    both_train, both_test, both_valid, both_y_train, both_y_test, both_y_val = sample_slices(combined, other_params)\n",
    "    \n",
    "    X_train_str.extend(ne_train)\n",
    "    X_val_str.extend(ne_valid)\n",
    "    X_test_str.extend(ne_test)\n",
    "    y_train_str.extend(ne_y_train)\n",
    "    y_val_str.extend(ne_y_val)\n",
    "    y_test_str.extend(ne_y_test)\n",
    "    \n",
    "    \n",
    "    X_train_both.extend(both_train)\n",
    "    X_val_both.extend(both_valid)\n",
    "    X_test_both.extend(both_test)\n",
    "    y_train_both.extend(both_y_train)\n",
    "    y_val_both.extend(both_y_val)\n",
    "    y_test_both.extend(both_y_test)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    \n",
    "print(np.vstack(X_train_str).shape, np.vstack(y_train_str).shape)\n",
    "print(np.vstack(X_val_str).shape, np.vstack(y_val_str).shape)\n",
    "print(np.vstack(X_test_str).shape, np.vstack(y_test_str).shape)\n",
    "\n",
    "print(np.stack(X_train_both).shape, np.stack(y_train_both).shape)\n",
    "print(np.stack(X_val_both).shape, np.stack(y_val_both).shape)\n",
    "print(np.stack(X_test_both).shape, np.stack(y_test_both).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa395543",
   "metadata": {},
   "source": [
    "Add the pulses to the respective groups, and close the HDF5 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f74c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_X_train_str = grp_str_train.create_dataset(\"X\", data=X_train_str)\n",
    "dset_y_train_str = grp_str_train.create_dataset(\"y\", data=y_train_str)\n",
    "\n",
    "dset_X_test_str = grp_str_test.create_dataset(\"X\", data=X_test_str)\n",
    "dset_y_test_str = grp_str_test.create_dataset(\"y\", data=y_test_str)\n",
    "\n",
    "dset_X_val_str = grp_str_val.create_dataset(\"X\", data=X_val_str)\n",
    "dset_y_val_str = grp_str_val.create_dataset(\"y\", data=y_val_str)\n",
    "\n",
    "\n",
    "dset_X_train_both = grp_both_train.create_dataset(\"X\", data=X_train_both)\n",
    "dset_y_train_both = grp_both_train.create_dataset(\"y\", data=y_train_both)\n",
    "\n",
    "dset_X_test_both = grp_both_test.create_dataset(\"X\", data=X_test_both)\n",
    "dset_y_test_both = grp_both_test.create_dataset(\"y\", data=y_test_both)\n",
    "\n",
    "dset_X_val_str = grp_both_val.create_dataset(\"X\", data=X_val_both)\n",
    "dset_y_val_str = grp_both_val.create_dataset(\"y\", data=y_val_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_group = f.create_group(\"meta\")\n",
    "pulse_list = np.array(list(set([int(key) for key in pulse_list])))\n",
    "key_list = np.array([s.encode('utf-8') for s in ALL_KEYS])\n",
    "meta_pulse = meta_group.create_dataset('pulse_list', data=pulse_set)\n",
    "meta_y_atrr = meta_group.create_dataset('y_column_names', data=key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e297a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed22e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../processed/pedestal_profile_dataset_v3.hdf5', 'r') as file:\n",
    "    group = file['density_and_temperature']\n",
    "    X_train, y_train = group['train']['X'][:], group['train']['y'][:]\n",
    "    # X, y = file['density_and_temperature']['X'][:], file['strohman']['y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee978b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59528, 2, 63), (59528, 14))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0fc43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
